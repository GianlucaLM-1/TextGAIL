model:
  attn_pdrop: 0.0
  embd_pdrop: 0.0
  initializer_range: 0.02
  layer_norm_epsilon: 1.0e-05
  n_ctx: 1024
  n_embd: 768
  n_head: 12
  n_layer: 12
  n_positions: 1024
  name: roberta-tokenized-gpt2
  output_attentions: false
  output_hidden_states: false
  output_past: true
  pad_token_id: 1
  resid_pdrop: 0.0
  vocab_size: 50265
task:
  name: CommonGEN
training:
  batch_size: 32
  checkpointing:
    directory: Checkpoints
    keep_checkpoint_every_num_seconds: 86400
    num_checkpoints_to_keep: 1000
    seconds_interval: -1
    steps_interval: -1
  logging:
    level: INFO
    seconds_interval: 2
    steps_interval: -1
  num_gpus_per_node: 1
  optimization:
    fp16: true
    fp16_opt_level: O1
    gradient_accumulation_steps: 1
    learning_rate: 2.0e-05
    max_gradient_norm: -1.0
    optimizer_name: AdamW
    warmup:
      scheduler_name: WarmupLinear
      warmup_steps: 1000
    weight_decay: 0.01
  random_seed: 123
  resume:
    resume: false
    resume_model: true
    resume_optimizer: true
    resume_rng_state: true
    resume_scheduler: true
  total_num:
    epochs: 3
    update_steps: -1
  validation:
    steps_interval: -1
